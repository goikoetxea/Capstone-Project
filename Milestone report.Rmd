---
title: "Data Science Capstone project: Milestone Report"
output: html_document
---


##Instructions

The goal of this project is just to display that you’ve gotten used to working with the data and that you are on track to create your prediction algorithm. Please submit a report on R Pubs (http://rpubs.com/) that explains your exploratory analysis and your goals for the eventual app and algorithm. This document should be concise and explain only the major features of the data you have identified and briefly summarize your plans for creating the prediction algorithm and Shiny app in a way that would be understandable to a non-data scientist manager. You should make use of tables and plots to illustrate important summaries of the data set. The motivation for this project is to:

1. Demonstrate that you’ve downloaded the data and have successfully loaded it in.
2. Create a basic report of summary statistics about the data sets.
3. Report any interesting findings that you amassed so far.
4. Get feedback on your plans for creating a prediction algorithm and Shiny app.


```{r, echo=TRUE}
library(stringi)
library(tm)
library(RWeka)
library(ggplot2)
```

### 1. Demonstrate that you´ve downloaded the data and sucessfully loaded it in.
```{r, eval=FALSE}
download.file("http://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip", "Coursera-Swiftkey.zip")
unzip("Coursera-Swiftkey.zip")
```

The unzipped folder contains four folders, each with data in a particular language: english, russian, german and finnish. Each of those folders, contains three files, with data originating from blogs, news and twitter, respectively. Our analysis, will focus on the english dataset: "en_US".
```{r, echo=TRUE}
list.files("final")
list.files("final/en_US")
```

### 2. Create a basic report of summary statistics about the data sets.
We now load the files contained in en_US and have a closer look at them (size, number of lines, number of characters and a summary of the number of words per line).

For twitter: 
```{r, echo=TRUE, warning=FALSE}
twitter <- readLines("final/en_US/en_US.twitter.txt", encoding="UTF-8", skipNul = TRUE)
file.info("final/en_US/en_US.twitter.txt")$size/1024/1024
stri_stats_general(twitter)
summary(stri_count_words(twitter))
```

For blogs: 
```{r, echo=TRUE, warning=FALSE}
blogs <- readLines("final/en_US/en_US.blogs.txt", encoding = "UTF-8")
file.info("final/en_US/en_US.blogs.txt")$size/1024/1024
stri_stats_general(blogs)
summary(stri_count_words(blogs))
```

For news: 
```{r, echo=TRUE, warning=FALSE}
news <- readLines("final/en_US/en_US.news.txt", encoding = "UTF-8")
file.info("final/en_US/en_US.news.txt")$size/1024/1024
stri_stats_general(news)
summary(stri_count_words(news))
```

Given that the files are too big, we will use a sample of length 10000 out of them:
```{r, echo=TRUE}
twitter_sample <- sample(twitter, 10000)
blogs_sample <- sample(blogs,10000)
news_sample <- sample(news, 10000)
```

Next, we will preprocess and clean the text. For this, we will remove punctuation, numbers, stopwords, swearwords ( taken from http://www.bannedwordlist.com/), change all chacracters to lower case and stripe white spaces. We do that for the tree files.

```{r, echo=TRUE}
swearwords<- readLines("~/Desktop/Coursera/Capstone Project/final/swearWords.txt", skipNul = TRUE)
blogs_corpus <- Corpus(VectorSource(list(blogs_sample)))
blogs_corpus <- tm_map(blogs_corpus, content_transformer(removePunctuation))
blogs_corpus <- tm_map(blogs_corpus, content_transformer(removeNumbers))
blogs_corpus <- tm_map(blogs_corpus, removeWords, stopwords("english"))
blogs_corpus <- tm_map(blogs_corpus, removeWords, swearwords)
blogs_corpus <- tm_map(blogs_corpus, content_transformer(tolower))
blogs_corpus <- tm_map(blogs_corpus, stripWhitespace)
```

```{r, echo=TRUE}
twitter_corpus <- Corpus(VectorSource(list(twitter_sample)))
twitter_corpus <- tm_map(twitter_corpus, content_transformer(removePunctuation))
twitter_corpus <- tm_map(twitter_corpus, content_transformer(removeNumbers))
twitter_corpus <- tm_map(twitter_corpus, removeWords, stopwords("english"))
twitter_corpus <- tm_map(twitter_corpus, removeWords, swearwords)
twitter_corpus <- tm_map(twitter_corpus, content_transformer(tolower))
twitter_corpus <- tm_map(twitter_corpus, stripWhitespace)
```

```{r, echo=TRUE}
new_corpus <- Corpus(VectorSource(list(news_sample)))
new_corpus <- tm_map(new_corpus, content_transformer(removePunctuation))
new_corpus <- tm_map(new_corpus, content_transformer(removeNumbers))
new_corpus <- tm_map(new_corpus, removeWords, stopwords("english"))
new_corpus <- tm_map(new_corpus, removeWords, swearwords)
new_corpus <- tm_map(new_corpus, content_transformer(tolower))
new_corpus <- tm_map(new_corpus, stripWhitespace)

```

### 3. Report any interesting findings that you amassed so far.
####Unigram Tokenizer
We now perform an unigram analysis to check which are the 20 most frequent words for each file.
```{r, echo=TRUE}
UnigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
tdm_blogs_unigram <- TermDocumentMatrix(blogs_corpus, control = list(tokenize = UnigramTokenizer))
blogs_unigram_df <- data.frame(terms = tdm_blogs_unigram$dimnames$Terms, frequency = tdm_blogs_unigram$v) 
blogs_unigram_df <- blogs_unigram_df[order(blogs_unigram_df$frequency, decreasing = TRUE),]
p1_blogs <- ggplot(head(blogs_unigram_df, 20), aes(x = reorder(terms, frequency), y= frequency)) + geom_bar(stat="identity") +
  xlab("Terms") + ylab("Frequency") + ggtitle("20 top unigrams in blogs") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
p1_blogs
```

```{r, echo=TRUE}
UnigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 1, max = 1))
tdm_twitter_unigram <- TermDocumentMatrix(twitter_corpus, control = list(tokenize = UnigramTokenizer))
twitter_unigram_df <- data.frame(terms = tdm_twitter_unigram$dimnames$Terms, frequency = tdm_twitter_unigram$v) 
twitter_unigram_df <- twitter_unigram_df[order(twitter_unigram_df$frequency, decreasing = TRUE),]
p1_twitter <- ggplot(head(twitter_unigram_df, 20), aes(x = reorder(terms, frequency), y= frequency)) + geom_bar(stat="identity") +
  xlab("Terms") + ylab("Frequency") + ggtitle("20 top unigrams in twitter") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
p1_twitter
```

For the twitter file, we also show a wordcloud plot.
```{r, echo=TRUE}
library(wordcloud)
wordcloud(head(twitter_unigram_df$terms,50), head(twitter_unigram_df$frequency,50))   
```

```{r, echo=TRUE}
tdm_news_unigram <- TermDocumentMatrix(new_corpus, control = list(tokenize = UnigramTokenizer))
news_unigram_df <- data.frame(terms = tdm_news_unigram$dimnames$Terms, frequency = tdm_news_unigram$v) 
news_unigram_df <- news_unigram_df[order(news_unigram_df$frequency, decreasing = TRUE),]
p1_news <- ggplot(head(news_unigram_df, 20), aes(x = reorder(terms, frequency), y= frequency)) + geom_bar(stat="identity") +
  xlab("Terms") + ylab("Frequency") + ggtitle("20 top unigrams in news") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
p1_news
```

### 4. Get feedback on your plans for creating a prediction algorithm and Shiny app.
Next steps will be to create a prediction algorithm. For this we will use bi-grams, 3-grams and 4-grams for predicting the next word. I will also try to use the Katz's Back-Off Model and see if it can be employed for this case. In addition, a more thorough cleaning can be made by using Stemming. I will analyse this option. 
